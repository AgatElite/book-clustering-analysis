{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqANhkFpZfqETMAhGBaiUe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgatElite/book-clustering-analysis/blob/main/Project_7_Stream_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA7lN7Z8r3hS"
      },
      "outputs": [],
      "source": [
        "# Install kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your kaggle.json file for authentication.\")\n",
        "# Upload the kaggle.json file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the file to the correct location\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "  # specific kaggle folder setup\n",
        "  !mkdir -p ~/.kaggle/\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "# Note: This matches the dataset link provided in the project description\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
        "!unzip -o amazon-books-reviews.zip\n",
        "\n",
        "import csv\n",
        "import math\n",
        "import hashlib\n",
        "import random\n",
        "import statistics\n",
        "import sys\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Set to True to process only a small portion (useful for debugging/testing code)\n",
        "# Set to False to process the stream \"for real\"\n",
        "DEBUG_MODE = False\n",
        "DEBUG_LIMIT = 50000\n",
        "\n",
        "# Dataset filename\n",
        "DATA_FILE = 'Books_rating.csv'\n",
        "\n",
        "print(\"Dataset downloaded and ready.\")"
      ],
      "metadata": {
        "id": "lylWoNLfsc7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stream(filename, limit=None):\n",
        "    \"\"\"\n",
        "    Generator that simulates a stream of data from the CSV file.\n",
        "    Yields one row at a time.\n",
        "    \"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        count = 0\n",
        "        for row in reader:\n",
        "            # We are interested in User_id for our stream analysis\n",
        "            user_id = row['User_id']\n",
        "            if user_id: # ensure it's not empty\n",
        "                yield user_id\n",
        "                count += 1\n",
        "                if limit and count >= limit:\n",
        "                    break\n",
        "\n",
        "def distinct_hash(s, index):\n",
        "    \"\"\"\n",
        "    Creates a family of hash functions using md5 and a salt (index).\n",
        "    Returns an integer.\n",
        "    \"\"\"\n",
        "    # We combine the string and the index to create independent hash functions\n",
        "    combined = f\"{s}-{index}\"\n",
        "    # Use md5 for better distribution than standard hash()\n",
        "    hex_digest = hashlib.md5(combined.encode('utf-8')).hexdigest()\n",
        "    # Convert hex to int\n",
        "    return int(hex_digest, 16)\n",
        "\n",
        "def trailing_zeros(n):\n",
        "    \"\"\"\n",
        "    Counts the number of trailing zeros in the binary representation of n.\n",
        "    Essential for Flajolet-Martin.\n",
        "    \"\"\"\n",
        "    if n == 0: return 0\n",
        "    s = bin(n)\n",
        "    # Count zeros from the right end\n",
        "    count = 0\n",
        "    for char in reversed(s):\n",
        "        if char == '0':\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "    return count"
      ],
      "metadata": {
        "id": "aflQDgPXselF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlajoletMartin:\n",
        "    def __init__(self, num_groups=5, hashes_per_group=4):\n",
        "        \"\"\"\n",
        "        Implementation of FM algorithm to estimate distinct elements (F0).\n",
        "        Uses median-of-averages to improve accuracy.\n",
        "        \"\"\"\n",
        "        self.num_groups = num_groups\n",
        "        self.hashes_per_group = hashes_per_group\n",
        "\n",
        "        # R_matrix stores the max trailing zeros for each hash function\n",
        "        # Structure: [group_0, group_1, ... ] where each group is a list of R values\n",
        "        self.R_matrix = [[0] * hashes_per_group for _ in range(num_groups)]\n",
        "\n",
        "    def process(self, item):\n",
        "        \"\"\"\n",
        "        Process a single item from the stream.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_groups):\n",
        "            for j in range(self.hashes_per_group):\n",
        "                # Unique index for the hash function\n",
        "                func_index = i * self.hashes_per_group + j\n",
        "\n",
        "                # Get hash value\n",
        "                h_val = distinct_hash(item, func_index)\n",
        "\n",
        "                # Count trailing zeros (r(x))\n",
        "                r = trailing_zeros(h_val)\n",
        "\n",
        "                # Maintain global variable R = max(R, r(x))\n",
        "                if r > self.R_matrix[i][j]:\n",
        "                    self.R_matrix[i][j] = r\n",
        "\n",
        "    def estimate(self):\n",
        "        \"\"\"\n",
        "        Returns the estimate of distinct elements.\n",
        "        Estimate ~ 2^R\n",
        "        \"\"\"\n",
        "        group_averages = []\n",
        "\n",
        "        for group in self.R_matrix:\n",
        "            # Calculate 2^R for each hash in the group\n",
        "            estimates = [2**r for r in group]\n",
        "            # Take the Average within the group\n",
        "            group_avg = statistics.mean(estimates)\n",
        "            group_averages.append(group_avg)\n",
        "\n",
        "        # Take the Median of the group averages to eliminate outliers\n",
        "        final_estimate = statistics.median(group_averages)\n",
        "        return int(final_estimate)"
      ],
      "metadata": {
        "id": "qJp29-mxsgko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AMS_Estimator:\n",
        "    def __init__(self, num_variables=100):\n",
        "        \"\"\"\n",
        "        Implementation of AMS Algorithm to estimate Second Moment (F2).\n",
        "        Uses Reservoir Sampling to handle infinite streams.\n",
        "        \"\"\"\n",
        "        self.num_variables = num_variables\n",
        "        # Our \"Reservoir\" of variables.\n",
        "        # Each variable stores: {'element': item, 'value': count}\n",
        "        self.variables = [None] * num_variables\n",
        "        self.n = 0 # Current stream length\n",
        "\n",
        "    def process(self, item):\n",
        "        \"\"\"\n",
        "        Process a new item from the stream.\n",
        "        \"\"\"\n",
        "        self.n += 1\n",
        "\n",
        "        # 1. Update existing counters\n",
        "        for i in range(self.num_variables):\n",
        "            if self.variables[i] is not None:\n",
        "                if self.variables[i]['element'] == item:\n",
        "                    self.variables[i]['value'] += 1\n",
        "\n",
        "        # 2. Reservoir Sampling logic for new positions\n",
        "        # We want to pick the current position (n) with probability num_variables / n\n",
        "        # (Technically we run this logic num_variables times or pick k positions,\n",
        "        # here we follow the \"Stream 4.5\" logic: maintain a sample set of size s)\n",
        "\n",
        "        # To simplify standard reservoir logic for AMS variables:\n",
        "        # We toss a coin to see if this current position 'n' should replace an existing variable.\n",
        "        # Probability to keep this item in reservoir of size s is s/n (approx logic for one slot)\n",
        "\n",
        "        # Standard implementation:\n",
        "        # Check if we should start a new counter for this item\n",
        "        # We have 'num_variables' slots.\n",
        "\n",
        "        # If reservoir is not full, fill it\n",
        "        for i in range(self.num_variables):\n",
        "            if self.variables[i] is None:\n",
        "                self.variables[i] = {'element': item, 'value': 1}\n",
        "                return\n",
        "\n",
        "        # If full, replace with probability s/n?\n",
        "        # Actually, standard AMS says: Pick position j uniform random.\n",
        "        # In a stream, we replace an existing generic variable with prob 1/n?\n",
        "        # Let's use the standard Reservoir sampling definition:\n",
        "        # With probability (num_variables / n), we decide to store this NEW item.\n",
        "        # If chosen, we evict a random one.\n",
        "\n",
        "        prob = self.num_variables / self.n\n",
        "        if random.random() < prob:\n",
        "            # Pick a victim to evict\n",
        "            victim_idx = random.randint(0, self.num_variables - 1)\n",
        "            # Replace it with new variable starting count at 1\n",
        "            self.variables[victim_idx] = {'element': item, 'value': 1}\n",
        "\n",
        "    def estimate(self):\n",
        "        \"\"\"\n",
        "        Calculate F2 estimate based on stored variables.\n",
        "        Y = n * (2 * v - 1)\n",
        "        \"\"\"\n",
        "        total_estimate = 0\n",
        "        valid_vars = 0\n",
        "\n",
        "        for var in self.variables:\n",
        "            if var is not None:\n",
        "                v = var['value']\n",
        "                # Estimator Y = n(2v - 1)\n",
        "                Y = self.n * (2 * v - 1)\n",
        "                total_estimate += Y\n",
        "                valid_vars += 1\n",
        "\n",
        "        if valid_vars == 0: return 0\n",
        "\n",
        "        # Average the estimates\n",
        "        return int(total_estimate / valid_vars)"
      ],
      "metadata": {
        "id": "4XxcuK2esija"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "# Initialize algorithms\n",
        "fm_algo = FlajoletMartin(num_groups=50, hashes_per_group=5)\n",
        "ams_algo = AMS_Estimator(num_variables=200)\n",
        "\n",
        "# Ground truth storage\n",
        "exact_counts = {}\n",
        "stream_len = 0\n",
        "\n",
        "# --- METRICS STORAGE ---\n",
        "history_x = []          # Stream position (n)\n",
        "history_fm_est = []     # F0 Estimate\n",
        "history_fm_true = []    # F0 True\n",
        "history_ams_est = []    # F2 Estimate\n",
        "history_ams_true = []   # F2 True\n",
        "timestamps = []         # For scalability analysis\n",
        "\n",
        "start_time = time.time()\n",
        "print(f\"Starting Stream Processing...\")\n",
        "\n",
        "# Process the stream\n",
        "# Note: Ensure DEBUG_MODE is False or DEBUG_LIMIT is high enough (e.g., 100k)\n",
        "# to get good looking graphs.\n",
        "stream = get_stream(DATA_FILE, limit=DEBUG_LIMIT if DEBUG_MODE else None)\n",
        "\n",
        "for user_id in stream:\n",
        "    stream_len += 1\n",
        "\n",
        "    # 1. Update Stream Algorithms\n",
        "    fm_algo.process(user_id)\n",
        "    ams_algo.process(user_id)\n",
        "\n",
        "    # 2. Update Ground Truth (Exact)\n",
        "    if user_id in exact_counts:\n",
        "        exact_counts[user_id] += 1\n",
        "    else:\n",
        "        exact_counts[user_id] = 1\n",
        "\n",
        "    # 3. Snapshot Metrics (every 1000 items to keep plot rendering fast)\n",
        "    if stream_len % 1000 == 0:\n",
        "        # F0 Snapshot\n",
        "        est_f0 = fm_algo.estimate()\n",
        "        true_f0 = len(exact_counts)\n",
        "\n",
        "        # F2 Snapshot\n",
        "        # Note: Calculating exact F2 is expensive, so we do it less frequently\n",
        "        # or we accept the slowdown for the sake of the report.\n",
        "        # For optimization, calculate true_f2 only every 5000 steps.\n",
        "        est_f2 = ams_algo.estimate()\n",
        "        true_f2 = sum([c**2 for c in exact_counts.values()])\n",
        "\n",
        "        history_x.append(stream_len)\n",
        "        history_fm_est.append(est_f0)\n",
        "        history_fm_true.append(true_f0)\n",
        "        history_ams_est.append(est_f2)\n",
        "        history_ams_true.append(true_f2)\n",
        "        timestamps.append(time.time() - start_time)\n",
        "\n",
        "    if stream_len % 10000 == 0:\n",
        "        print(f\"Processed {stream_len} reviews... Current F0 Est: {est_f0}\")\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"Processing Complete. Generating Plots...\")\n",
        "\n",
        "# --- PLOTTING FOR REPORT ---\n",
        "\n",
        "# 1. Flajolet-Martin Convergence Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history_x, history_fm_true, label='Ground Truth (Exact)', color='black', linestyle='--')\n",
        "plt.plot(history_x, history_fm_est, label='FM Estimate', color='blue', alpha=0.7)\n",
        "plt.title('Count Distinct (F0): Estimate vs Truth')\n",
        "plt.xlabel('Stream Items Processed')\n",
        "plt.ylabel('Count of Distinct Users')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('f0_convergence.png') # Save for LaTeX\n",
        "plt.show()\n",
        "\n",
        "# 2. AMS Convergence Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history_x, history_ams_true, label='Ground Truth (Exact)', color='black', linestyle='--')\n",
        "plt.plot(history_x, history_ams_est, label='AMS Estimate', color='red', alpha=0.7)\n",
        "plt.title('Second Moment (F2): Estimate vs Truth')\n",
        "plt.xlabel('Stream Items Processed')\n",
        "plt.ylabel('Second Moment Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('f2_convergence.png') # Save for LaTeX\n",
        "plt.show()\n",
        "\n",
        "# 3. Scalability (Time vs Input Size)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history_x, timestamps, color='green', marker='o', markersize=2)\n",
        "plt.title('Scalability: Execution Time vs Stream Size')\n",
        "plt.xlabel('Stream Items Processed')\n",
        "plt.ylabel('Time (seconds)')\n",
        "plt.grid(True)\n",
        "plt.savefig('scalability_time.png') # Save for LaTeX\n",
        "plt.show()\n",
        "\n",
        "print(\"Plots saved as .png files. Download them from the files tab!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzu9rGw9skiT",
        "outputId": "f4396057-e5ba-4204-9303-93685f1eccf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stream Processing...\n",
            "Processed 10000 reviews... Current F0 Est: 19456\n",
            "Processed 20000 reviews... Current F0 Est: 31129\n",
            "Processed 30000 reviews... Current F0 Est: 43827\n",
            "Processed 40000 reviews... Current F0 Est: 54886\n",
            "Processed 50000 reviews... Current F0 Est: 60620\n",
            "Processed 60000 reviews... Current F0 Est: 80281\n",
            "Processed 70000 reviews... Current F0 Est: 121241\n",
            "Processed 80000 reviews... Current F0 Est: 141721\n",
            "Processed 90000 reviews... Current F0 Est: 147456\n",
            "Processed 100000 reviews... Current F0 Est: 154009\n",
            "Processed 110000 reviews... Current F0 Est: 167116\n",
            "Processed 120000 reviews... Current F0 Est: 183500\n",
            "Processed 130000 reviews... Current F0 Est: 262144\n",
            "Processed 140000 reviews... Current F0 Est: 271974\n",
            "Processed 150000 reviews... Current F0 Est: 271974\n",
            "Processed 160000 reviews... Current F0 Est: 275251\n",
            "Processed 170000 reviews... Current F0 Est: 281804\n",
            "Processed 180000 reviews... Current F0 Est: 298188\n",
            "Processed 190000 reviews... Current F0 Est: 308019\n",
            "Processed 200000 reviews... Current F0 Est: 314572\n",
            "Processed 210000 reviews... Current F0 Est: 353894\n",
            "Processed 220000 reviews... Current F0 Est: 363724\n",
            "Processed 230000 reviews... Current F0 Est: 363724\n",
            "Processed 240000 reviews... Current F0 Est: 373555\n",
            "Processed 250000 reviews... Current F0 Est: 373555\n",
            "Processed 260000 reviews... Current F0 Est: 373555\n",
            "Processed 270000 reviews... Current F0 Est: 373555\n",
            "Processed 280000 reviews... Current F0 Est: 373555\n",
            "Processed 290000 reviews... Current F0 Est: 386662\n",
            "Processed 300000 reviews... Current F0 Est: 386662\n",
            "Processed 310000 reviews... Current F0 Est: 386662\n",
            "Processed 320000 reviews... Current F0 Est: 399769\n",
            "Processed 330000 reviews... Current F0 Est: 452198\n",
            "Processed 340000 reviews... Current F0 Est: 452198\n",
            "Processed 350000 reviews... Current F0 Est: 478412\n",
            "Processed 360000 reviews... Current F0 Est: 478412\n",
            "Processed 370000 reviews... Current F0 Est: 478412\n",
            "Processed 380000 reviews... Current F0 Est: 478412\n"
          ]
        }
      ]
    }
  ]
}