{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEn47FKrnbu4A/+slUpp+1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgatElite/book-clustering-analysis/blob/main/Project_7_Stream_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA7lN7Z8r3hS"
      },
      "outputs": [],
      "source": [
        "# Install kaggle library\n",
        "!pip install -q kaggle\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Please upload your kaggle.json file for authentication.\")\n",
        "# Upload the kaggle.json file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move the file to the correct location\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "  # specific kaggle folder setup\n",
        "  !mkdir -p ~/.kaggle/\n",
        "  !cp kaggle.json ~/.kaggle/\n",
        "  !chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "# Note: This matches the dataset link provided in the project description\n",
        "!kaggle datasets download -d mohamedbakhet/amazon-books-reviews\n",
        "!unzip -o amazon-books-reviews.zip\n",
        "\n",
        "import csv\n",
        "import math\n",
        "import hashlib\n",
        "import random\n",
        "import statistics\n",
        "import sys\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Set to True to process only a small portion (useful for debugging/testing code)\n",
        "# Set to False to process the stream \"for real\"\n",
        "DEBUG_MODE = True\n",
        "DEBUG_LIMIT = 50000\n",
        "\n",
        "# Dataset filename\n",
        "DATA_FILE = 'Books_rating.csv'\n",
        "\n",
        "print(\"Dataset downloaded and ready.\")"
      ],
      "metadata": {
        "id": "lylWoNLfsc7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stream(filename, limit=None):\n",
        "    \"\"\"\n",
        "    Generator that simulates a stream of data from the CSV file.\n",
        "    Yields one row at a time.\n",
        "    \"\"\"\n",
        "    with open(filename, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        count = 0\n",
        "        for row in reader:\n",
        "            # We are interested in User_id for our stream analysis\n",
        "            user_id = row['User_id']\n",
        "            if user_id: # ensure it's not empty\n",
        "                yield user_id\n",
        "                count += 1\n",
        "                if limit and count >= limit:\n",
        "                    break\n",
        "\n",
        "def distinct_hash(s, index):\n",
        "    \"\"\"\n",
        "    Creates a family of hash functions using md5 and a salt (index).\n",
        "    Returns an integer.\n",
        "    \"\"\"\n",
        "    # We combine the string and the index to create independent hash functions\n",
        "    combined = f\"{s}-{index}\"\n",
        "    # Use md5 for better distribution than standard hash()\n",
        "    hex_digest = hashlib.md5(combined.encode('utf-8')).hexdigest()\n",
        "    # Convert hex to int\n",
        "    return int(hex_digest, 16)\n",
        "\n",
        "def trailing_zeros(n):\n",
        "    \"\"\"\n",
        "    Counts the number of trailing zeros in the binary representation of n.\n",
        "    Essential for Flajolet-Martin.\n",
        "    \"\"\"\n",
        "    if n == 0: return 0\n",
        "    s = bin(n)\n",
        "    # Count zeros from the right end\n",
        "    count = 0\n",
        "    for char in reversed(s):\n",
        "        if char == '0':\n",
        "            count += 1\n",
        "        else:\n",
        "            break\n",
        "    return count"
      ],
      "metadata": {
        "id": "aflQDgPXselF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FlajoletMartin:\n",
        "    def __init__(self, num_groups=5, hashes_per_group=4):\n",
        "        \"\"\"\n",
        "        Implementation of FM algorithm to estimate distinct elements (F0).\n",
        "        Uses median-of-averages to improve accuracy.\n",
        "        \"\"\"\n",
        "        self.num_groups = num_groups\n",
        "        self.hashes_per_group = hashes_per_group\n",
        "\n",
        "        # R_matrix stores the max trailing zeros for each hash function\n",
        "        # Structure: [group_0, group_1, ... ] where each group is a list of R values\n",
        "        self.R_matrix = [[0] * hashes_per_group for _ in range(num_groups)]\n",
        "\n",
        "    def process(self, item):\n",
        "        \"\"\"\n",
        "        Process a single item from the stream.\n",
        "        \"\"\"\n",
        "        for i in range(self.num_groups):\n",
        "            for j in range(self.hashes_per_group):\n",
        "                # Unique index for the hash function\n",
        "                func_index = i * self.hashes_per_group + j\n",
        "\n",
        "                # Get hash value\n",
        "                h_val = distinct_hash(item, func_index)\n",
        "\n",
        "                # Count trailing zeros (r(x))\n",
        "                r = trailing_zeros(h_val)\n",
        "\n",
        "                # Maintain global variable R = max(R, r(x))\n",
        "                if r > self.R_matrix[i][j]:\n",
        "                    self.R_matrix[i][j] = r\n",
        "\n",
        "    def estimate(self):\n",
        "        \"\"\"\n",
        "        Returns the estimate of distinct elements.\n",
        "        Estimate ~ 2^R\n",
        "        \"\"\"\n",
        "        group_averages = []\n",
        "\n",
        "        for group in self.R_matrix:\n",
        "            # Calculate 2^R for each hash in the group\n",
        "            estimates = [2**r for r in group]\n",
        "            # Take the Average within the group\n",
        "            group_avg = statistics.mean(estimates)\n",
        "            group_averages.append(group_avg)\n",
        "\n",
        "        # Take the Median of the group averages to eliminate outliers\n",
        "        final_estimate = statistics.median(group_averages)\n",
        "        return int(final_estimate)"
      ],
      "metadata": {
        "id": "qJp29-mxsgko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AMS_Estimator:\n",
        "    def __init__(self, num_variables=100):\n",
        "        \"\"\"\n",
        "        Implementation of AMS Algorithm to estimate Second Moment (F2).\n",
        "        Uses Reservoir Sampling to handle infinite streams.\n",
        "        \"\"\"\n",
        "        self.num_variables = num_variables\n",
        "        # Our \"Reservoir\" of variables.\n",
        "        # Each variable stores: {'element': item, 'value': count}\n",
        "        self.variables = [None] * num_variables\n",
        "        self.n = 0 # Current stream length\n",
        "\n",
        "    def process(self, item):\n",
        "        \"\"\"\n",
        "        Process a new item from the stream.\n",
        "        \"\"\"\n",
        "        self.n += 1\n",
        "\n",
        "        # 1. Update existing counters\n",
        "        for i in range(self.num_variables):\n",
        "            if self.variables[i] is not None:\n",
        "                if self.variables[i]['element'] == item:\n",
        "                    self.variables[i]['value'] += 1\n",
        "\n",
        "        # 2. Reservoir Sampling logic for new positions\n",
        "        # We want to pick the current position (n) with probability num_variables / n\n",
        "        # (Technically we run this logic num_variables times or pick k positions,\n",
        "        # here we follow the \"Stream 4.5\" logic: maintain a sample set of size s)\n",
        "\n",
        "        # To simplify standard reservoir logic for AMS variables:\n",
        "        # We toss a coin to see if this current position 'n' should replace an existing variable.\n",
        "        # Probability to keep this item in reservoir of size s is s/n (approx logic for one slot)\n",
        "\n",
        "        # Standard implementation:\n",
        "        # Check if we should start a new counter for this item\n",
        "        # We have 'num_variables' slots.\n",
        "\n",
        "        # If reservoir is not full, fill it\n",
        "        for i in range(self.num_variables):\n",
        "            if self.variables[i] is None:\n",
        "                self.variables[i] = {'element': item, 'value': 1}\n",
        "                return\n",
        "\n",
        "        # If full, replace with probability s/n?\n",
        "        # Actually, standard AMS says: Pick position j uniform random.\n",
        "        # In a stream, we replace an existing generic variable with prob 1/n?\n",
        "        # Let's use the standard Reservoir sampling definition:\n",
        "        # With probability (num_variables / n), we decide to store this NEW item.\n",
        "        # If chosen, we evict a random one.\n",
        "\n",
        "        prob = self.num_variables / self.n\n",
        "        if random.random() < prob:\n",
        "            # Pick a victim to evict\n",
        "            victim_idx = random.randint(0, self.num_variables - 1)\n",
        "            # Replace it with new variable starting count at 1\n",
        "            self.variables[victim_idx] = {'element': item, 'value': 1}\n",
        "\n",
        "    def estimate(self):\n",
        "        \"\"\"\n",
        "        Calculate F2 estimate based on stored variables.\n",
        "        Y = n * (2 * v - 1)\n",
        "        \"\"\"\n",
        "        total_estimate = 0\n",
        "        valid_vars = 0\n",
        "\n",
        "        for var in self.variables:\n",
        "            if var is not None:\n",
        "                v = var['value']\n",
        "                # Estimator Y = n(2v - 1)\n",
        "                Y = self.n * (2 * v - 1)\n",
        "                total_estimate += Y\n",
        "                valid_vars += 1\n",
        "\n",
        "        if valid_vars == 0: return 0\n",
        "\n",
        "        # Average the estimates\n",
        "        return int(total_estimate / valid_vars)"
      ],
      "metadata": {
        "id": "4XxcuK2esija"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize algorithms\n",
        "fm_algo = FlajoletMartin(num_groups=5, hashes_per_group=5)\n",
        "ams_algo = AMS_Estimator(num_variables=200)\n",
        "\n",
        "# Ground truth storage (for comparison)\n",
        "exact_counts = {}\n",
        "stream_len = 0\n",
        "\n",
        "print(f\"Starting Stream Processing (Debug Mode: {DEBUG_MODE})...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Process the stream\n",
        "stream = get_stream(DATA_FILE, limit=DEBUG_LIMIT if DEBUG_MODE else None)\n",
        "\n",
        "for user_id in stream:\n",
        "    stream_len += 1\n",
        "\n",
        "    # 1. Update Stream Algorithms\n",
        "    fm_algo.process(user_id)\n",
        "    ams_algo.process(user_id)\n",
        "\n",
        "    # 2. Update Ground Truth (Exact)\n",
        "    if user_id in exact_counts:\n",
        "        exact_counts[user_id] += 1\n",
        "    else:\n",
        "        exact_counts[user_id] = 1\n",
        "\n",
        "    # Optional: print status every 10k items\n",
        "    if stream_len % 10000 == 0:\n",
        "        print(f\"Processed {stream_len} reviews...\")\n",
        "\n",
        "# --- RESULTS ---\n",
        "print(\"-\" * 60)\n",
        "print(f\"Stream processing complete. Total Length (F1): {stream_len}\")\n",
        "\n",
        "# 1. Distinct Elements (F0)\n",
        "exact_f0 = len(exact_counts)\n",
        "est_f0 = fm_algo.estimate()\n",
        "error_f0 = abs(exact_f0 - est_f0) / exact_f0 * 100\n",
        "\n",
        "print(f\"\\n[F0] Distinct Users Analysis:\")\n",
        "print(f\"  Exact Count (Ground Truth): {exact_f0}\")\n",
        "print(f\"  Flajolet-Martin Estimate:   {est_f0}\")\n",
        "print(f\"  Error Rate:                 {error_f0:.2f}%\")\n",
        "\n",
        "# 2. Second Moment (F2)\n",
        "exact_f2 = sum([c**2 for c in exact_counts.values()])\n",
        "est_f2 = ams_algo.estimate()\n",
        "error_f2 = abs(exact_f2 - est_f2) / exact_f2 * 100\n",
        "\n",
        "print(f\"\\n[F2] Surprise Number (Second Moment) Analysis:\")\n",
        "print(f\"  Exact F2 (Ground Truth):    {exact_f2}\")\n",
        "print(f\"  AMS Estimate:               {est_f2}\")\n",
        "print(f\"  Error Rate:                 {error_f2:.2f}%\")\n",
        "\n",
        "print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "pzu9rGw9skiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}